#%%
# https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009
import numpy as np # linear algebra
from sklearn.model_selection import GridSearchCV
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.ensemble import GradientBoostingClassifier

print('Done')
#%% md
# Introductory Materials


#%%
DATA = pd.read_csv("input/data.csv")
DATA.info() 
#%% md
# Data quick preparation

#%%
# Ids = DATA.Id
# DATA.drop('Id', axis = 'columns', inplace=True)
DATA.info()
#%% md

# Exploring the Data, planning the preprocessing

#%% md
## Features Overview

#%%
from libs.simpleplotter import simple_features_overview
simple_features_overview(DATA)
#%% md
# Correlations between data
#%%
# Heatmap
from libs.simpleplotter import simple_heatmap
# simple_heatmap(DATA)
#%% md

#%% md
##
#%%
# sns.catplot(x="SepalLengthCm",y="Species",data=DATA)
from libs.simpleplotter import simple_correlations
# simple_correlations(DATA, "quality")

#%% md
## Missing, Categorical & Not Useful Data

#%%
# DATA.describe(include='object')
#%%
DATA.isnull().sum()
#%%
DATA.head()
#%% md

#%% md

# Data Processing

#%%
# Let's run the preprocessing on both train and test data

PROCESSED = DATA.copy(deep=True)

PROCESSED.dropna(axis='index', subset=PROCESSED.columns, inplace=True)

# PROCESSED["Na"] = PROCESSED["Na"]/10
# PROCESSED["Si"] = PROCESSED["Si"]/30
# PROCESSED["Mg"] = PROCESSED["Mg"]/4
# PROCESSED["Ca"] = PROCESSED["Ca"]/8
# PROCESSED["RI"] = PROCESSED["RI"]/1.5

print("Done")
# df['Age'].fillna(df["Age"].mean(), inplace=True)
# fill missing embarked values with the most common one
# df['Embarked'].fillna('S', inplace=True)
# we don't need whole age, as experiments show that age < 18
# is most important for survival
# df["is_child"] = df["Age"]<18

# cols = ['Name', 'Cabin', 'Ticket', 'PassengerId']
# df.drop(cols, axis=1, inplace=True)
# for the need of Receiver Operating Characteristic (ROC) diagram we need to convert strings to ints
# PROCESSED.replace('Iris-setosa', 1, inplace=True)
# PROCESSED.replace('Iris-versicolor', 2, inplace=True)
# PROCESSED.replace('Iris-virginica', 3, inplace=True)
#%% md

#%%
PROCESSED.head()
#%% md

# Post-Processing Data Analysis

#%% md

#%%
# Heatmap
from libs.simpleplotter import simple_heatmap
# simple_heatmap(PROCESSED)
#%% md
# Preparing for Model Training

## Separating target from features
#%%
target = 'quality'
y = PROCESSED[target]
X = PROCESSED.drop([target], axis=1)
X.info()
#%% md
## Splitting train data into train & validation data
as we can see the number of records in train data is lowered
#%%
X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)
X_train.info()
#%%
X_valid.info()
#%% md
# Models Training & Hyper-params Tuning for Different Classification Models

#%%
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from libs.simple_processing import get_model_name, print_scores
from pandas import DataFrame
from sklearn.gaussian_process import GaussianProcessClassifier
import imblearn
from sklearn.linear_model import LogisticRegression
from sklearn.linear_model import SGDClassifier

params = [

#     # 0.8011627906976744
#     # {'max_depth': 10, 'n_estimators': 90}
#     # {
#     #     'model': RandomForestClassifier(criterion='entropy', n_jobs=3, max_features="auto", bootstrap=False),
#     #     'hyperparams': {
#     #         'n_estimators': range(20, 100, 10),
#     #         'max_depth': range(4, 10, 1)
#     #     }
#     # }
#     # 0.8069767441860465
#     # {'max_depth': 8, 'n_estimators': 120, 'random_state': 10}
#     # {
#     #     'model': RandomForestClassifier(criterion='entropy', n_jobs=3, max_features="auto", bootstrap=True),
#     #     'hyperparams': {
#     #         'random_state': range(0, 60, 10),
#     #         'n_estimators': range(80, 150, 10),
#     #         'max_depth': range(7, 15, 1)
#     #     }
#     # }
#     # 0.8012311901504787
#     # {'ccp_alpha': 0.0, 'max_depth': 7, 'max_samples': 50, 'n_estimators': 110, 'random_state': 0}
#     # {
#     #     'model': RandomForestClassifier(criterion='entropy', n_jobs=3, max_features="auto", bootstrap=True),
#     #     'hyperparams': {
#     #         'ccp_alpha': np.arange(0, 1, 0.2),
#     #         'max_samples': range(10, 80, 10),
#     #         'random_state': range(0, 30, 5),
#     #         'n_estimators': range(100, 140, 10),
#     #         'max_depth': range(7, 10, 1)
#     #     }
#     # }
#     # 0.8012995896032832
#     # {'max_depth': 6, 'max_samples': 65, 'n_estimators': 100, 'random_state': 0}
#     # {
#     #     'model': RandomForestClassifier(criterion='entropy', n_jobs=3, max_features="auto", bootstrap=True),
#     #     'hyperparams': {
#     #         'max_samples': range(40, 70, 5),
#     #         'random_state': range(0, 30, 5),
#     #         'n_estimators': range(100, 130, 10),
#     #         'max_depth': range(5, 9, 1)
#     #     }
#     # }
#     # 0.8012995896032832
#     # {'max_depth': 6, 'max_samples': 65, 'n_estimators': 60}
#     # {
#     #     'model': RandomForestClassifier(criterion='entropy', n_jobs=3, max_features="auto", bootstrap=True, random_state=0),
#     #     'hyperparams': {
#     #         'max_samples': range(50, 80, 5),
#     #         'n_estimators': range(40, 80, 10),
#     #         'max_depth': range(5, 9, 1)
#     #     }
#     # }
#     # 0.8012995896032832
#     # {'max_depth': 6, 'max_features': 'auto', 'max_samples': 65, 'n_estimators': 60}
#     # {
#     #     'model': RandomForestClassifier(criterion='entropy', n_jobs=3, bootstrap=True, oob_score=True, random_state=0),
#     #     'hyperparams': {
#     #         'max_features':["auto", "sqrt", "log2"],
#     #         'max_samples': range(50, 80, 5),
#     #         'n_estimators': range(40, 80, 10),
#     #         'max_depth': range(4, 8, 1)
#     #     }
#     # }
#     # 0.7370041039671682
#     # {'learning_rate': 0.09, 'max_depth': 6, 'n_estimators': 11, 'random_state': 5}
#     # {
#     #     'model': GradientBoostingClassifier(max_features="auto"),
#     #     'hyperparams': {
#     #         # 'criterion': ['friedman_mse', 'mse', 'mae'],
#     #         'random_state': range(0, 30, 5),
#     #         'n_estimators': range(5, 15, 1),
#     #         'learning_rate': np.arange(0.01, 0.1, 0.01),
#     #         'max_depth': range(4, 8, 1)
#     #     }
#     # }
#     # 0.7954856361149111
#     # {'learning_rate': 1.0199999999999996, 'max_depth': 5, 'n_estimators': 12, 'random_state': 10}
#     # {
#     #     'model': GradientBoostingClassifier(max_features="auto"),
#     #     'hyperparams': {
#     #         # 'criterion': ['friedman_mse', 'mse', 'mae'],
#     #         'random_state': range(0, 15, 5),
#     #         'n_estimators': range(10, 13, 1),
#     #         'learning_rate': np.arange(0.07, 1.2, 0.01),
#     #         'max_depth': range(5, 8, 1)
#     #     }
#     # }
#     # 0.8185362517099863
#     # {'learning_rate': 1.2, 'max_depth': 6, 'n_estimators': 80, 'random_state': 6}
#     # {
#     #     'model': GradientBoostingClassifier(max_features="auto"),
#     #     'hyperparams': {
#     #         # 'criterion': ['friedman_mse', 'mse', 'mae'],
#     #         'random_state': range(0, 10, 2),
#     #         'n_estimators': range(20, 100, 10),
#     #         'learning_rate': np.arange(0.9, 1.5, 0.1),
#     #         'max_depth': range(4, 12, 2)
#     #     }
#     # }
#     # 0.8245554035567715
#     # {'criterion': 'mse', 'learning_rate': 1.2000000000000002, 'max_depth': 6, 'n_estimators': 75, 'random_state': 6}
#     {
#         'model': GradientBoostingClassifier(loss='deviance', max_features="auto"),
#         'hyperparams': {
#             'criterion': ['friedman_mse', 'mse', 'mae'],
#             # 'loss':['deviance', 'exponential'],
#             'random_state': range(5, 8, 1),
#             'n_estimators': range(60, 90, 5),
#             'learning_rate': np.arange(1, 1.4, 0.1),
#             'max_depth': range(4, 9, 1)
#         }
#     }
#     # {'learning_rate': 1.2000000000000002, 'max_depth': 7, 'n_estimators': 81, 'random_state': 5}
#     # {
#     #     'model': GradientBoostingClassifier(criterion='mse', loss='deviance', max_features="auto"),
#     #     'hyperparams': {
#     #         # 'criterion': ['friedman_mse', 'mse', 'mae'],
#     #         # 'loss':['deviance', 'exponential'],
#     #         'random_state': range(0, 10, 2),
#     #         'n_estimators': range(80, 85, 1),
#     #         'learning_rate': np.arange(1, 1.4, 0.1),
#     #         'max_depth': range(6, 11, 1)
#     #     }
#     # }
#     # cv=3
#     # train / test: 50 / 50
#     {
#         'model': GradientBoostingClassifier(criterion='mse', loss='deviance', max_features="auto"),
#         'hyperparams': {
#             'random_state': range(0, 10, 2),
#             'n_estimators': range(80, 83, 1),
#             'learning_rate': np.arange(1, 1.4, 0.1),
#             'max_depth': range(6, 9, 1)
#         }
#     }
#         {
#             'model': SMOTE(loss='deviance', criterion='mse', random_state=6, n_estimators=75, learning_rate=1.2, max_depth=6, max_features='log2'),
#             'hyperparams': {
#                 'max_features': ['auto', 'log2', 2, 6, 10]
#             }
#         }
        # MULTICLASS:

    # 0.5160309076682316
    # {'copy_X_train': True, 'max_iter_predict': 1, 'multi_class': 'one_vs_one', 'n_restarts_optimizer': 0, 'random_state': 0, 'warm_start': False}
    #     {
    #         'model': GaussianProcessClassifier(),
    #         'hyperparams': {
    #             'copy_X_train': [True, False],
    #             'multi_class': ['one_vs_rest', 'one_vs_one'],
    #             'random_state': range(0, 3, 1),
    #             'n_restarts_optimizer': range(0, 3, 1),
    #             'max_iter_predict': range(0, 3, 1),
    #             'warm_start': [True, False]
    #         }
    #     }
    # 0.5676386443661972
    # {'C': 1.2000000000000002, 'multi_class': 'auto', 'penalty': 'l2', 'random_state': 0, 'solver': 'newton-cg', 'warm_start': True}
    #     {
    #         'model': LogisticRegression(),
    #         'hyperparams': {
    #             'C': np.arange(0.6, 1.4, 0.2),
    #             'multi_class': ['auto', 'ovr', 'warn'],
    #             'random_state': range(0, 12, 2),
    #             'penalty': ['l2'],
    #             'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],
    #             'warm_start': [True, False]
    #         }
    #     }
    # 0.5871752450980392
    # {'C': 1.4, 'multi_class': 'auto', 'penalty': 'l2', 'random_state': 0, 'warm_start': True}, cv=5
    #     {
    #         'model': LogisticRegression(solver='newton-cg'),
    #         'hyperparams': {
    #             'C': np.arange(1, 2, 0.2),
    #             'multi_class': ['auto', 'ovr', 'warn'],
    #             'random_state': range(0, 8, 2),
    #             'penalty': ['l1', 'l2'],
    #             # 'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],
    #             'warm_start': [True, False]
    #         }
    #     }
    # 0.5660834800469483
    # {'eta0': 0.03, 'fit_intercept': True, 'learning_rate': 'adaptive', 'loss': 'hinge', 'max_iter': 500, 'penalty': 'l1', 'random_state': None, 'shuffle': False}
    #     {
    #         'model': SGDClassifier(),
    #         'hyperparams': {
    #             # 'alpha': np.arange(0.0001, 0.001, 0.0003),
    #             # 'tol': np.arange(0.001, 0.01, 0.003),
    #             'eta0': np.arange(0.0, 0.1, 0.03),
    #             'loss': ['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron'],
    #             'max_iter': range(500, 1500, 200),
    #             'random_state': [None, 0, 4, 10, 20],
    #             'penalty': ['l1', 'l2'],
    #             # 'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],
    #             'learning_rate': ['constant', 'optimal', 'invscaling', 'adaptive'],
    #             'shuffle': [True, False],
    #             'fit_intercept': [True, False]
    #         }
    #     }
    # 0.5723359252738653
    # {'alpha': 0.00030000000000000003, 'fit_intercept': True, 'learning_rate': 'adaptive', 'loss': 'log', 'shuffle': False, 'tol': 0.002}
    #     {
    #         'model': SGDClassifier(eta0=0.03, penalty='l1', max_iter=55000, random_state=None),
    #         'hyperparams': {
    #             'alpha': np.arange(0.0001, 0.0005, 0.0001),
    #             'tol': np.arange(0.001, 0.005, 0.001),
    #             'loss': ['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron'],
    #             # 'max_iter': range(500, 1500, 200),
    #             # 'random_state': [None, 0, 4, 10, 20],
    #             # 'penalty': ['l1', 'l2'],
    #             'learning_rate': ['constant', 'optimal', 'invscaling', 'adaptive'],
    #             'shuffle': [True, False],
    #             'fit_intercept': [True, False]
    #         }
    #     }
    
    # 0.612987822769953
    # {'criterion': 'friedman_mse', 'learning_rate': 1.0, 'max_depth': 5, 'n_estimators': 80, 'random_state': 7}    
    #     {
    #         'model': GradientBoostingClassifier(loss='deviance', max_features="auto"),
    #         'hyperparams': {
    #             'criterion': ['friedman_mse', 'mse', 'absolute_error'],
    #             # 'loss':['deviance', 'exponential'],
    #             'random_state': range(5, 15, 2),
    #             'n_estimators': range(75, 85, 2),
    #             'learning_rate': np.arange(0.8, 1.2, 0.1),
    #             'max_depth': range(4, 7, 1)
    #         }
    #     }
        {
            'model': GradientBoostingClassifier(max_features="auto"),
            'hyperparams': {
                'criterion': ['friedman_mse', 'mse', 'absolute_error'],
                'loss':['deviance', 'exponential'],
                'random_state': range(5, 15, 2),
                'n_estimators': range(75, 85, 2),
                'learning_rate': np.arange(0.8, 1.2, 0.1),
                'max_depth': range(4, 7, 1)
            }
        }
]
#
# # https://medium.com/all-things-ai/in-depth-parameter-tuning-for-gradient-boosting-3363992e9bae
#
grid_model = {}

for p in params:
    print('> Model:', get_model_name(p['model']))
    # grid_model = GridSearchCV(p['model'], p['hyperparams'], cv=2, n_jobs=3, verbose=1, error_score='raise')
    grid_model = GridSearchCV(p['model'], p['hyperparams'], cv=2, n_jobs=3, verbose=1)
    grid_model.fit(X_train, y_train)
    print(grid_model.best_score_)
    print(grid_model.best_params_)
    y_pred = grid_model.predict(X_valid)
    df = DataFrame(y_valid, y_pred)
    print(df)


print('Done.')
#%%
# model = {}
#
# # model = GradientBoostingClassifier(criterion='mse', learning_rate=1.2, max_depth=6, n_estimators=75)
#
# for lr in np.arange(0.8, 1.4, 0.1):
#     for md in range(4, 12, 1):
#         for ne in range(70, 120, 5):
#             model = GradientBoostingClassifier(criterion='mse', learning_rate=1.2, max_depth=md, n_estimators=ne)
#             model.fit(X_train, y_train)
#             print('lr: {:.2f}'.format(lr) + ' md: {:.0f}'.format(md) + ' ne: {:.0f}'.format(ne)
#                   + ' => train data score: {:.3f}'.format(model.score(X_train, y_train))
#                 + ' val data score: {:.3f}'.format(model.score(X_valid, y_valid)))

#%%
# from libs.single_scoring import get_scoring
# parameters = [
# #     params:  {'n_estimators': 450, 'learning_rate': 0.035, 'max_depth': 6}
# # => train data: 0.973 val data: 0.716
# #     {
# #         'modelName': globals()["GradientBoostingClassifier"],
# #         'rnames': ['n_estimators', 'learning_rate', 'max_depth'],
# #         'rvalues': [range(150, 500, 100), np.arange(0.02, 0.04, 0.005), range(4, 7, 1)],
# #         'pnames': [],
# #         'pvalues': []
# #     }
#     {
#         'modelName': globals()["GradientBoostingClassifier"],
#         'rnames': ['n_estimators', 'learning_rate', 'max_depth'],
#         'rvalues': [range(350, 1000, 100), np.arange(0.03, 0.08, 0.01), range(5, 9, 1)],
#         'pnames': [],
#         'pvalues': []
#     }
#     # {
#     #     'modelName': GradientBoostingClassifier(),
#     #     'rnames': ['n_estimators', 'learning_rate', 'max_depth'],
#     #     'rvalues': [range(350, 1000, 100), np.arange(0.03, 0.08, 0.01), range(5, 9, 1)],
#     #     'pnames': ['criterion'],
#     #     'pvalues': ["mse"]
#     # }
# ]
#
# for p in parameters:
#     score = get_scoring(X_train, y_train, X_valid, y_valid, p['modelName'], p['rnames'], p['rvalues'], p['pnames'], p['pvalues'])
#
#
print("Done")

#%%
from libs.single_scoring import get_scoring
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.gaussian_process import GaussianProcessClassifier

# @TODO
# https://www.projectpro.io/article/multi-class-classification-python-example/547

parameters = [
    #     params:  {'n_estimators': 450, 'learning_rate': 0.035, 'max_depth': 6}
    # => train data: 0.973 val data: 0.716
    #     {
    #         'modelName': globals()["GradientBoostingClassifier"],
    #         'rnames': ['n_estimators', 'learning_rate', 'max_depth'],
    #         'rvalues': [range(150, 500, 100), np.arange(0.02, 0.04, 0.005), range(4, 7, 1)],
    #         'pnames': [],
    #         'pvalues': []
    #     }
    # {
    #     'n_estimators': range(200, 600, 100),
    #     'learning_rate': np.arange(0.03, 0.07, 0.01),
    #     'max_depth': range(5, 9, 1)
    # },
#     BEST params:  {'n_estimators': 90, 'learning_rate': 0.04, 'max_depth': 8}
# => train data: 1.000 val data: 0.731
#     {
#         'modelName': globals()["GradientBoostingClassifier"],
#         'hyperParamNames': ['n_estimators', 'learning_rate', 'max_depth'],
#         'hyperParamValues': [range(10, 100, 10), np.arange(0.03, 0.07, 0.01), range(5, 9, 1)],
#         'params': {}
#     }
#     BEST params:  {'n_estimators': 200, 'learning_rate': 0.02, 'max_depth': 7}
# => train data: 1.000 val data: 0.713
#     {
#         'modelName': globals()["GradientBoostingClassifier"],
#         'hyperParamNames': ['n_estimators', 'learning_rate', 'max_depth'],
#         'hyperParamValues': [range(50, 300, 50), np.arange(0.01, 0.04, 0.01), range(5, 8, 1)],
#         'params': {'criterion': "mse", "n_jobs": 3}
#     }
#     BEST params:  {'n_estimators': 90, 'learning_rate': 1.0, 'max_depth': 7}
# => train data: 1.000 val data: 0.713
# {
#         'modelName': globals()["GradientBoostingClassifier"],
#         'hyperParamNames': ['n_estimators', 'learning_rate', 'max_depth'],
#         'hyperParamValues': [range(60, 120, 10), np.arange(1, 1.4, 0.1), range(5, 8, 1)],
#         'params': {'max_features': "auto", "n_jobs": 3, 'criterion': 'mse'}
#     }
#     BEST params:  {'n_estimators': 90, 'learning_rate': 0.6, 'max_depth': 9}
# => train data: 1.000 val data: 0.750
#     {
#         'modelName': globals()["GradientBoostingClassifier"],
#         'hyperParamNames': ['n_estimators', 'learning_rate', 'max_depth'],
#         'hyperParamValues': [range(60, 120, 10), np.arange(0.5, 1, 0.1), range(6, 10, 1)],
#         'params': {'max_features': "auto", "n_jobs": 3, 'criterion': 'mse'}
#     }
#     BEST params:  {'n_estimators': 90, 'learning_rate': 0.7, 'max_depth': 11}
# => train data: 1.000 val data: 0.756
#     {
#         'modelName': globals()["GradientBoostingClassifier"],
#         'hyperParamNames': ['n_estimators', 'learning_rate', 'max_depth'],
#         'hyperParamValues': [range(80, 110, 10), np.arange(0.5, 0.8, 0.1), range(8, 12, 1)],
#         'params': {'max_features': "auto", "n_jobs": 3, 'criterion': 'mse'}
#     }
#     BEST params:  {'n_estimators': 100, 'learning_rate': 0.7, 'max_depth': 11}
# => train data: 1.000 val data: 0.753
#     BEST params:  {'n_estimators': 130, 'learning_rate': 0.8999999999999999, 'max_depth': 11}
# => train data: 1.000 val data: 0.738
#     model = RandomForestClassifier(criterion='gini', max_depth=7, n_estimators=150, n_jobs=4, random_state=0)

#     RandomForestClassifier
#     BEST params:  {'n_estimators': 80, 'bootstrap': False, 'max_depth': 10}
# => train data: 0.973 val data: 0.734
#     {
#         'modelName': globals()["RandomForestClassifier"],
#         'hyperParamNames': ['n_estimators', 'bootstrap', 'max_depth'],
#         'hyperParamValues': [range(80, 140, 10), [True, False], range(7, 11, 1)],
#         'params': {'random_state': 0, "n_jobs": 3, 'criterion': 'gini'}
#     }
#     BEST params:  {'n_estimators': 80, 'criterion': 'gini', 'max_depth': 15}
# => train data: 1.000 val data: 0.747
#     BEST params:  {'n_estimators': 50, 'max_leaf_nodes': 26, 'max_depth': 12}
# => train data: 0.720 val data: 0.694
#     {
#         'modelName': globals()["RandomForestClassifier"],
#         'hyperParamNames': ['n_estimators', 'max_leaf_nodes', 'max_depth'],
#         'hyperParamValues': [range(50, 100, 10), range(2, 30, 2), range(10, 30, 2)],
#         'params': {"n_jobs": 3, 'bootstrap': True, 'criterion': 'gini', 'random_state': 0}
#     }
#     BEST params:  {'n_estimators': 30, 'max_leaf_nodes': 18, 'max_depth': 8}
# => train data: 0.695 val data: 0.681
#     {
#         'modelName': globals()["RandomForestClassifier"],
#         'hyperParamNames': ['n_estimators', 'max_leaf_nodes', 'max_depth'],
#         'hyperParamValues': [range(20, 70, 10), range(2, 30, 2), range(4, 10, 2)],
#         'params': {"n_jobs": 3, 'bootstrap': True, 'criterion': 'gini', 'random_state': 0}
#     }
#     BEST params:  {'n_estimators': 41, 'max_leaf_nodes': 19, 'max_depth': 8}
# => train data: 0.672 val data: 0.684
#     {
#         'modelName': globals()["RandomForestClassifier"],
#         'hyperParamNames': ['n_estimators', 'max_leaf_nodes', 'max_depth'],
#         'hyperParamValues': [range(39, 43, 1), range(12, 20, 1), range(7, 10, 1)],
#         'params': {"n_jobs": 3, 'bootstrap': True, 'criterion': 'gini', 'random_state': 0}
#     }
#     BEST params:  {'n_estimators': 120, 'bootstrap': False, 'max_depth': 10}
# => train data: 0.974 val data: 0.741
#     {
#         'modelName': globals()["RandomForestClassifier"],
#         'hyperParamNames': ['n_estimators', 'bootstrap', 'max_depth'],
#         'hyperParamValues': [range(80, 140, 10), [True, False], range(7, 11, 1)],
#         'params': {'random_state': 0, "n_jobs": 3, 'criterion': 'gini'}
#     }
#     BEST params:  {'n_estimators': 110, 'bootstrap': False, 'max_depth': 9}
# => train data: 0.934 val data: 0.731
#     {
#         'modelName': globals()["RandomForestClassifier"],
#         'hyperParamNames': ['n_estimators', 'bootstrap', 'max_depth'],
#         'hyperParamValues': [range(80, 140, 10), [True, False], range(7, 10, 1)],
#         'params': {'random_state': 0, "n_jobs": 3, 'criterion': 'gini', 'max_leaf_nodes': 19}
#     }
#     BEST params:  {'C': 0.8, 'penalty': 'l2', 'solver': 'liblinear'}
# => train data: 0.574 val data: 0.628
#     {
#         'modelName': globals()["LogisticRegression"],
#         'hyperParamNames': ['C', 'penalty', 'solver'],
#         'hyperParamValues': [np.arange(0.6, 1.4, 0.2), ['l2'], ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']],
#         'params': {'random_state': 8, "n_jobs": 3}
#     }
#     {
#         'modelName': globals()["LogisticRegression"],
#         'hyperParamNames': ['solver', 'penalty', 'max_iter'],
#         'hyperParamValues': [['saga'], ['l1', 'elasticnet'], range(60, 140, 10)],
#         'params': {'random_state': '8', "n_jobs": 3, 'C': '0.8', "l1_ratio": "0.5"}
#     }
    {
        'modelName': globals()["GaussianProcessClassifier"],
        'hyperParamNames': ["random_state", "n_restarts_optimizer", "max_iter_predict"],
        'hyperParamValues': [range(0, 6, 2), range(0, 6, 2), range(50, 120, 20)],
        'params': {"multi_class": "one_vs_one", "warm_start": True, "n_jobs": 3}
    }
]

for p in parameters:
    score = get_scoring(X_train, y_train, X_valid, y_valid, p['modelName'], p['hyperParamNames'], p['hyperParamValues'])


print("Done")

#%% md
## Score

#%%
# FROM GRIDSEARCHCV

    # 0.8245554035567715
#     # {'criterion': 'mse', 'learning_rate': 1.2000000000000002, 'max_depth': 6, 'n_estimators': 75, 'random_state': 6}
# {'criterion': 'mse', 'learning_rate': 1.2000000000000002, 'max_depth': 6, 'n_estimators': 75, 'random_state': 6}

#     BEST params:  {'n_estimators': 90, 'learning_rate': 0.7, 'max_depth': 11}
# => train data: 1.000 val data: 0.756
#     {
#         'modelName': globals()["GradientBoostingClassifier"],
#         'hyperParamNames': ['n_estimators', 'learning_rate', 'max_depth'],
#         'hyperParamValues': [range(80, 110, 10), np.arange(0.5, 0.8, 0.1), range(8, 12, 1)],
#         'params': {'max_features': "auto", "n_jobs": 3, 'criterion': 'mse'}
#     }

model = GradientBoostingClassifier(max_features="auto", criterion='mse', learning_rate=0.7, max_depth=11, n_estimators=90, random_state=6)
model.fit(X_train, y_train)
# print_scores(y_valid, y_pred)
print('train Score: ', model.score(X_train, y_train))
print('val Score: ', model.score(X_valid, y_valid))
#%%
from sklearn.linear_model import LogisticRegression

model = LogisticRegression()
model.fit(X_train, y_train)
# print_scores(y_valid, y_pred)
print('train Score: ', model.score(X_train, y_train))
print('val Score: ', model.score(X_valid, y_valid))
#%%
# BEST params:  {'n_estimators': 110, 'bootstrap': False, 'max_depth': 9}
# => train data: 0.934 val data: 0.731
# {
#     'modelName': globals()["RandomForestClassifier"],
#     'hyperParamNames': ['n_estimators', 'bootstrap', 'max_depth'],
#     'hyperParamValues': [range(80, 140, 10), [True, False], range(7, 10, 1)],
#     'params': {'random_state': 0, "n_jobs": 3, 'criterion': 'gini', 'max_leaf_nodes': 19}
# }
model = RandomForestClassifier(criterion='gini', max_depth=9, n_estimators=110, n_jobs=3, random_state=0, max_leaf_nodes=19)
model.fit(X_train, y_train)
# y_pred = model.predict(X_valid)
# print_scores(y_valid, y_pred)
print('train Score: ', model.score(X_train, y_train))
print('val Score: ', model.score(X_valid, y_valid))
#%%
from libs.simple_processing import print_scores

model = KNeighborsClassifier(n_jobs=4, n_neighbors=19)
model.fit(X_train, y_train)
y_pred = model.predict(X_valid)
# print_scores(y_valid, y_pred)
print('train Score: ', model.score(X_train, y_train))
print('val Score: ', model.score(X_valid, y_valid))
#%%
model = GaussianNB()
model.fit(X_train, y_train)
# y_pred = model.predict(X_valid)
print('train Score: ', model.score(X_train, y_train))
print('val Score: ', model.score(X_valid, y_valid))
#%%
from libs.simpleplotter import decision_tree
model = DecisionTreeClassifier(max_depth=9, random_state=0, max_leaf_nodes=9 )
model.fit(X_train, y_train)
# y_pred = model.predict(X_valid)
# print_scores(y_valid, y_pred)
print('train Score: ', model.score(X_train, y_train))
print('val Score: ', model.score(X_valid, y_valid))
# decision_tree(model, X_train)
#%%
# BEST params:  {'n_estimators': 130, 'learning_rate': 0.8999999999999999, 'max_depth': 11}
# => train data: 1.000 val data: 0.738
model = GradientBoostingClassifier(max_features="auto", learning_rate=1.2, max_depth=6, n_estimators=80)
model.fit(X_train, y_train)
# print_scores(y_valid, y_pred)
print('train Score: ', model.score(X_train, y_train))
print('val Score: ', model.score(X_valid, y_valid))

#%% md
# Winner
#%%
model = GradientBoostingClassifier(max_features="auto", criterion='mse', learning_rate=0.7, max_depth=11, n_estimators=90, random_state=6)
model.fit(X_train, y_train)
# print_scores(y_valid, y_pred)
print('train Score: ', model.score(X_train, y_train))
print('val Score: ', model.score(X_valid, y_valid))

#%% md
## Receiver Operating Characteristic (ROC)
#%%
from libs.simpleplotter import simple_roc
y_pred = model.predict(X_valid)

df = DataFrame(y_valid, y_pred)
df
# simple_roc(y_valid, y_pred)
#%% md
## Confusion Matrix / Macierz pomyłek
#%%
from libs.simpleplotter import simple_confusion_matrix
conf_matrix = simple_confusion_matrix(y_valid, y_pred, model.classes_)
#%% md
## Which features became decision makers for the model?
#%%
from libs.simpleplotter import feature_importance

feature_importance(model, X_train)
#%% md
# Generating Model Predictions For Test Data & Saving the results for Kaggle Competition
#%%
y_pred = model.predict(X)
output = pd.DataFrame({'Id': Ids, 'Species': y_pred})

print_scores(y, y_pred)
# output.to_csv('./submission.csv', index=False)
# SUBMISSION = pd.read_csv("./submission.csv")
# SUBMISSION